{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78621f7d",
   "metadata": {},
   "source": [
    "#### Get 'The Verdict' Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac735184",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\") as the_verdict_file:\n",
    "  the_verdict_raw_text = the_verdict_file.read() # Read in text from 'The Verdict'\n",
    "\n",
    "print(the_verdict_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028cd798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(the_verdict_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e79584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported regular expressions!\n"
     ]
    }
   ],
   "source": [
    "# Import regular expressions\n",
    "import re\n",
    "print(\"Imported regular expressions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dd4b4",
   "metadata": {},
   "source": [
    "#### Tokenize 'The Verdict', removing white space, punctuation, and treating individual words as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98010ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "# Remove all punctuation and white space\n",
    "# Regular expression supplied by the video\n",
    "the_verdict_tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', the_verdict_raw_text)\n",
    "the_verdict_tokens = [token for token in the_verdict_tokens if token.strip()]\n",
    "print(len(the_verdict_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045603fc",
   "metadata": {},
   "source": [
    "#### Create a Vocabulary out the words in 'The Verdict'\n",
    "#### Assign them each a Unique Token Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771f625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(the_verdict_tokens)) # Get only unique tokens, and sort them alphabetically\n",
    "vocabulary_size = len(all_words)\n",
    "\n",
    "# Go over every token and assign them a unique token id using enumerate\n",
    "token_to_id_mapping = {token:token_id for token_id, token in enumerate(all_words)} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602db8d",
   "metadata": {},
   "source": [
    "#### Create your own version of the Version One Tokenizer\n",
    "To Note:\n",
    "* It can take in a sample text in \\_\\_init\\_\\_(...)\n",
    "* It can then create a mapping dictionary from tokens to ids using the \\_\\_init\\_\\_(...) function\n",
    "* For the function encode_text(...), it can use the token to ids mapping dictionary to take in a string containing a sentence, and then output a list of integer ids\n",
    "* For the function decode_ids(...), it can use the token to ids mapping dictionary to take in a list of ids encoding a sentence, and then output the corresponding list of tokens. You can also specify a parameter to output the sentence as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create byte pair algorithm. This will occur later in the video\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
