{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55400ec4",
   "metadata": {},
   "source": [
    "####  Exploring Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca74ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4494, 1.2856, 1.3202, 1.3484, 1.2090, 0.8788])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "inputs = torch.tensor([\n",
    "  [0.46, 0.86, 0.83],\n",
    "  [0.36, 0.74, 0.78],\n",
    "  [0.37, 0.94, 0.63],\n",
    "  [0.64, 0.52, 0.94],\n",
    "  [0.96, 0.84, 0.31],\n",
    "  [0.53, 0.74, 0.18],\n",
    "])\n",
    "\n",
    "query_token = inputs[1]\n",
    "#print(inputs.shape)\n",
    "\n",
    "## This vector holds the dot product results between each embedding vector\n",
    "## for each token in our context and the embedding vector of our query token,\n",
    "## our query token being the current token we're using to try and predict what word\n",
    "## should come next.\n",
    "attention_score_vector = torch.empty(inputs.shape[0])\n",
    "#print(\"Attention\", attention_result_vector)\n",
    "\n",
    "for index, tensor in enumerate(inputs):\n",
    "  attention_score_vector[index] = torch.dot(query_token, inputs[index])\n",
    "\n",
    "print(torch.round(attention_score_vector, decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7487b60",
   "metadata": {},
   "source": [
    "#### Now, we wish to normalize the attention score vector\n",
    "##### Helps with optimization - to work with smaller numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b3e410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4494, 1.2856, 1.3202, 1.3484, 1.2090, 0.8788])\n",
      "tensor([0.1935, 0.1716, 0.1762, 0.1800, 0.1614, 0.1173])\n",
      "Torch softmax'd tensor([0.2007, 0.1704, 0.1764, 0.1814, 0.1578, 0.1134])\n"
     ]
    }
   ],
   "source": [
    "attention_score_vector_normalized = attention_score_vector/torch.sum(attention_score_vector)\n",
    "print(attention_score_vector)\n",
    "print(attention_score_vector_normalized)\n",
    "\n",
    "\n",
    "# This is a more stable version of softmax based normalization\n",
    "# Softmax formula: e^{x_i} / sum_[of all x_i] (e^{x_i})\n",
    "torch_normalized_attention_score = torch.softmax(attention_score_vector, dim=0)\n",
    "print(\"Torch softmax'd\", torch_normalized_attention_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202d376",
   "metadata": {},
   "source": [
    "#### Now we want to go through each value of our normalized attention score vector, multiply it to each embedding vector in our inputs array, and then add the product, individually, to an all-zeros vector, creating our context vector for the second token, our query token.\n",
    "\n",
    "##### attention_score_vector_unnormalized: query_token [of shape (d)], dot_product_with_each_input_token_embedding_vector[of shape (n, d)], creates new vector of shape (n,)\n",
    "\n",
    "##### attention_score_vector: normalized version of attention_score_vector_unnormalized, using torch.softmax(vector, dim=0), on the unnormalized vector\n",
    "\n",
    "##### context vector: take each element in your normalized attention score vector [of shape (n,)] for the current query token, multiply it by the corresponding input token embedding vector [the array of embedding vectors is of shape (n, d)], and add it to a vector initially filled with all zeros [of shape (d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeaca97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3600, 0.7400, 0.7800])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
